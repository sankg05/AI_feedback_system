The objective of this task was to evaluate how different prompting strategies affect the performance of a Large Language Model (LLM) when classifying Yelp reviews into 1â€“5 star ratings. Instead of fine-tuning a model, the task focuses on prompt engineering, treating the LLM as a black-box classifier and analyzing how prompt design impacts:
Prediction accuracy
Structured JSON output validity
Reliability and consistency of responses
